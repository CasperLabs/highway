\documentclass[12pt, fleqn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsfonts, amsthm, amsmath, amssymb}
\usepackage{stix}
\usepackage{a4wide}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage{biblatex}

\addbibresource{highway.bib}

\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    citecolor=blue,
    linkcolor=blue,
}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\title{The CasperLabs Highway Protocol}
\author[1]{Daniel Kane}
\author[2]{Vlad Zamfir}
\author[3]{Andreas Fackler}

\affil[1]{Computer Science and Engineering Department, UC San Diego}
\affil[2]{Ethereum Research}
\affil[3]{CasperLabs LLC}

\date{September 2019}

\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}

\newcommand{\ww}{\mathbb{w}}

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\maketitle
\tableofcontents
\pagebreak


\begin{abstract}
We present Highway, a consensus algorithm based on CBC Casper together with a criterion for finality, both proposed by Daniel Kane. This algorithm is live in a partially synchronous network, if the participants have approximately synchronized clocks. Highway has variable out-of-protocol finality thresholds each observer can choose independently, as a tradeoff between safety and liveness.
\end{abstract}


\section*{Introduction}

Fault tolerant protocols for state machine replication are at the heart of distributed databases, cryptocurrencies and smart contract platforms: mechanisms that allow a network to reach consensus on the state and progress of a decentralized virtual computer, even if some of its nodes malfunction or get compromised.

Classical solutions like PBFT \cite{castro1999practical} usually have a well-defined threshold for what \emph{number} of faulty nodes can be tolerated, and are meant to be used in a \emph{permissioned} context, with an immutable set of participating nodes, fixed from the beginning.

Bitcoin \cite{nakamoto2008bitcoin} introduced a \emph{permissionless} Proof-of-Work consensus algorithm, where the set of participating nodes is continually changing, and anyone can join or leave at any time, and Ethereum \cite{wood2014ethereum} extended this concept from a pure cryptocurrency to a general programmable smart contract platform. Both networks have very high value and correspondingly high requirements for security, due to potentially large incentives for attackers.

Proof-of-Work inherently has extremely high energy requirements \cite{de2018bitcoin}. Proof-of-Stake networks (e.g. \cite{kiayias2017ouroboros}) attempt to address this issue by giving the power to make consensus decisions not to the nodes that contribute the most computing resources, but to the ones that deposit the highest stakes as a collateral. They aspire to provide the same or higher levels of security and performance, while requiring less energy by many orders of magnitude.


\subsection*{Our Contribution}

Highway is a concrete consensus algorithm based on CBC Casper \cite{zamfir2018casper} together with a criterion for finality, both proposed by Daniel Kane. It is live in a partially synchronous network, if the participants have clocks that are synchronized up to an unknown but bounded difference.

At the heart of Casper we made two design choices that keep it simple and intuitive:
\begin{itemize}
  \item There are no targeted messages sent only to specific nodes, like e.g. \cite{miller2016honey}. Instead, all messages are ``broadcasted'' to all network nodes and they attest to earlier messages; all nodes see the same, continuously growing graph of messages, similar to \cite{baird2016hashgraph,chevalier2018parsec,gkagol2019aleph,moser1999byzantine}.
  \item Decisions are made using simple, explicit voting: You always have to follow the plurality.\footnote{This is in contrast to protocols like \cite{buchman2018latest,castro1999practical,muratov2018yac} where a leader proposes and if not enough confirmations are collected, a new leader retries with a different proposal, or e.g. \cite{kiayias2017ouroboros}, where the longest chain is always selected.}
\end{itemize}

The Highway protocol ensures liveness by enforcing a particular structure in the graph that prevents stalemates from persisting.
It allows participating nodes to have different voting weights, making it suitable for Proof-of-Stake networks, where a node's weight is proportional to its stake. Moreover, Highway has variable out-of-protocol finality thresholds each observer can choose independently, thus allowing individual tradeoffs between safety and liveness.

As a partially synchronous protocol --- it creates messages according to a schedule that relies on real time, so it is not fully asynchronous ---, Highway is not subject to the FLP impossibility \cite{fischer1982impossibility} and does not rely on randomness for liveness.


\subsection*{Overview}

In section \ref{sectionMessages} we introduce the basic concepts of messages, justifications and equivocations, and analyze the safety guarantees that can be derived from their structure so that we can determine in a general setting when consensus decisions are final, i.e. under some reasonable assumptions they cannot be reverted.

Section \ref{sectionBlockchain} provides an explanation about how these results can be applied to the specific use case of a blockchain.

In section \ref{sectionLiveness}, we introduce additional rules that guarantee not only safety, but also liveness; decisions can be relied on and are eventually actually made.

Finally, section \ref{sectionPermissionless} puts the abstract consensus algorithm in the context of a distributed proof of stake network, outlines how deposits, rewards and penalties can be handled, and addresses the problem of long range attacks.

\newpage


\section{Messages and Finality}
\label{sectionMessages}


\subsection{Justifications}

In Casper, every message $\mu$ sent by an honest validator $v$ contains attestations of $v$ to messages sent or received before. These earlier messages are referred to as \emph{justifications}, and we say that $\mu$ \emph{cites} them.

In theory, this approach could be implemented by including all previous messages in $\mu$. In practice, we include the hashes of the previous messages, omitting redundant ones: if message $\mu_3$ cites $\mu_2$, and $\mu_2$ cites $\mu_1$, then $\mu_3$ does not need to explicitly include $\mu_1$'s hash anymore. Nevertheless, we consider $\mu_1$ a justification of $\mu_3$. Conversely, if $\mu_1$ is a justification of $\mu_2$, all justifications of $\mu_1$ are also justifications of $\mu_2$.  In summary:

\begin{definition}
  Given a set $\mathcal{M}$ of \emph{messages}, a \emph{justification function} is a function $J: \mathcal{M} \rightarrow \mathcal{P}(\mathcal{M})$, such that:
  \begin{itemize}
    \item For every $\mu \in \mathcal{M}$, $J(\mu)$ is finite.
    \item Whenever $\mu \in J(\lambda)$, then $J(\mu) \subseteq J(\lambda)$.
  \end{itemize}
  The elements of $J(\mu)$ are the \emph{justifications} of $\mu$. We also write $\mu < \lambda$ for $\mu \in J(\lambda)$, and say that $\lambda$ \emph{cites} $\mu$.

  A finite set $\sigma \subseteq \mathcal{M}$ is called a \emph{protocol state} if it is closed under $J$, i.e. if $J(\mu) \subseteq \sigma$ for every $\mu \in \sigma$. We denote the set of all protocol states as $\Sigma_{\mathcal{M}}$, or just $\Sigma$, if $\mathcal{M}$ is clear from the context.
\end{definition}

The relation $<$ is a well-founded\footnote{i.e. every nonempty set has a minimal element. This is true because for every message $\mu$, the set $\{ \lambda \mid \lambda < \mu \} = J(\mu)$ is finite, so there are no infinite descending sequences.} strict partial order\footnote{It is irreflexive because $\mu \notin J(\mu)$ and transitive because $J(\mu) \subseteq J(\lambda)$ whenever $\mu < \lambda$.} on $\mathcal{M}$. We use the usual notation for partial orders, e.g. $\mu \leq \lambda$ means $\mu \in J(\lambda) \cup \{\lambda\}$. We also say $\lambda$ \emph{sees} $\mu$ if $\mu \leq \lambda$.

Note that $J(\mu)$ and $\bar{J}(\mu) = J(\mu) \cup \{\mu\}$ are protocol states for every message $\mu$.

In a network, every node maintains its own current protocol state that includes all messages it sent and received so far --- with the exception of messages $\mu$ for which some justification $\lambda \in J(\mu)$ has not yet been received. Those are kept in a temporary storage while the node requests $\lambda$ from its peers. That means a node's protocol state is monotonically increasing; if it has state $\sigma$ before it has state $\tau$, then $\sigma \subseteq \tau$.

Moreover, we assume that all correct nodes will continuously sync with each other and exchange all messages they know of. So, whenever two honest nodes are in states $\sigma$ and $\sigma'$, they will eventually reach states $\tau$ and $\tau'$ such that $\sigma \cup \sigma' \subseteq \tau$ and $\sigma \cup \sigma' \subseteq \tau'$.


\subsection{Equivocations}

Messages are cryptographically signed by and contain the name (or ID) of their sender. Together with the justifications, this allows us to enforce many aspects of correct behavior on the protocol level, and declare as invalid the messages that do not follow those behavior rules.

Unless otherwise specified, we will implicitly assume a fixed set $\mathcal{M}$ of messages, together with a justification function $J$, and a function $S: \mathcal{M} \rightarrow \mathcal{V}$ that assigns its sender $S(\mu)$ to each message $\mu$ from some set $\mathcal{V}$ of \emph{validators}.

\begin{definition}
Given a validator $v \in \mathcal{V}$ and a protocol state $\sigma$, we call the set $\mathrm{Swim}_v(\sigma) = \{ \mu \in \sigma \mid S(\mu) = v \}$ of all of $v$'s messages in $\sigma$, the \emph{swimlane of $v$ in $\sigma$}.
\end{definition}

When creating a new message $\mu$, a validator $v$ is expected to always include all of $v$'s previous messages in $J(\mu)$ .  An honest validator's swimlane $\mathrm{Swim}_v(\sigma) = \{\mu_1, \mu_2, \mu_3, \ldots \}$ will always be totally ordered chronologically: $\mu_1 < \mu_2 < \mu_3 < \ldots$. Violating this rule means producing two messages that don't see each other:

\begin{definition}
A pair of messages $\mu$ and $\lambda$ is an \emph{equivocation}, if $S(\mu) = S(\lambda)$ and $\mu \not\leq \lambda$ and $\lambda \not\leq \mu$. The sender $v = S(\mu)$ is an \emph{equivocator}. For every protocol state $\sigma$, let
  $$E(\sigma) = \{v\in\mathcal{V} \mid \text{There exist } \mu, \lambda \in \mathrm{Swim}_v(\sigma), \text{ such that } \mu \not\leq \lambda \text{ and } \lambda \not\leq \mu\}$$
be the set of validators who produced an equivocation that is contained in $\sigma$.
\end{definition}

Since honest nodes will eventually exchange all messages they know of, any equivocation will eventually become known to all honest nodes. And, if $\sigma \subseteq \tau$, then $E(\sigma) \subseteq E(\tau)$.

We write $\nu \sqsubset \mu$ if $\nu < \mu$ and $S(\nu) = S(\mu)$.


\subsection{Votes and Weights}

The protocol will support a changing set of active validators with different weights, but we will study this aspect later. For now, we consider a fixed map:

\begin{definition}
  A \emph{validator map} is a map $\ww: \mathcal{V} \rightarrow \mathbb{R}_{\geq 0}$, assigning to each validator $v \in \mathcal{V}$ a \emph{weight} $\ww(v)$, such that at least one, but only finitely many validators are assigned a positive weight\footnote{To simplify the notation, instead of changing the set $\mathcal{V}$ itself, we will only change the map $w$, and assign $0$ to all validators who are not currently active. So, conceptually $\mathcal{V}$ is the set of all \emph{potential} validators and can even be infinite. In an implementation, $w$ should be represented as the finite map containing only the non-zero entries.}.

  The \emph{weight} of a subset $V \subseteq \mathcal{V}$ of validators is the sum of its members' weights: $\ww(V) = \sum_{v \in V} \ww(v)$, and the \emph{weight} of a set of messages $X \subseteq \mathcal{M}$ is the sum of the weight of their senders (counting each sender only once, even if they have multiple messages):
  $$\ww(X) = \ww(S(X)) \;\text{, where } S(X) = \{ S(\mu) \mid \mu \in X\}$$
\end{definition}

A central part of Casper's design,  messages count as votes for consensus values, and validators are required to follow the plurality of the non-equivocating validators' latest votes. However, in practice every message will count as a vote for several different things simultaneously. Specifically, in the case of blockchain a message containing a block is always also a vote for all of that block's ancestors.

To capture this notion, we will consider functions $f: \mathcal{M} \rightarrow \mathcal{C} \cup \{ \mathbb{0} \}$, assigning to each message $\mu$ a value $f(\mu) \in \mathcal{C}$ that the message is voting for, or a special value $\mathbb{0} \notin \mathcal{C}$ if the message does not carry a vote, i.e. counts as an abstention. To tally the votes, we will need to look at each validator's latest non-abstention message.

\begin{definition}
  The set of \emph{latest honest messages with property\footnote{Properties can be represented as a subset $p \subseteq \mathcal{M}$. We say that $p$ \emph{applies to $\mu$}, or just $p(\mu)$, if $\mu \in p$. We write $\neg p$ for "not $p$", i.e. $\neg p(\mu)$ is true if and only if $p(\mu)$ is not.} $p$}
  $$L_p(\sigma) = \{ \mu \in \sigma \mid S(\mu) \notin E(\sigma) \text{ and } p(\mu) \text{ and for all } \lambda \in \mathrm{Swim}_{S(\mu)}(\sigma), \lambda \leq \mu \text{ or } \neg p(\lambda) \}$$
contains the latest messages with property $p$ of all non-equivocating validators. We also call a message \emph{$p$-message} if it has property $p$.

  For a message $\mu$, we define $L_p(\mu) = L_p(J(\mu))$ for brevity, and we write $L$ without a subscript for the latest honest messages with the always-true property, i.e.:
  $$L(\sigma) = \{ \mu \in \sigma \mid S(\mu) \notin E(\sigma) \text{ and for all } \lambda \in \mathrm{Swim}_{S(\mu)}(\sigma), \lambda \leq \mu \}$$
Recall that non-equivocating validators' swimlanes are totally ordered, so $L(\sigma)$ contains exactly the maxima of their swimlanes.

  Given a voting function $f: \mathcal{M} \rightarrow \mathcal{C} \cup \{ \mathbb{0} \}$ with a totally ordered set $\mathcal{C}$, let
  $$L_f(\sigma) \;=\; \{ \mu \in \sigma \mid S(\mu) \notin E(\sigma) \text{ and } f(\mu) \neq \mathbb{0} \text{ and for all } \lambda \in \mathrm{Swim}_{S(\mu)}(\sigma), f(\lambda) = \mathbb{0} \text{ or } \lambda \leq \mu \}\text{,}$$
  i.e. $L_f = L_p$ for the property $p(\mu) \Leftrightarrow f(\mu) \neq \mathbb{0}$. Let
  $$W_f(c, \sigma) \;=\; \ww(\{ \mu \in L_f(\sigma) \mid f(\mu) = c\})$$
  for $c \in \mathcal{C}$: the total weight of the non-equivocating validators whose latest non-$\mathbb{0}$ vote was for $c$.

  $f$ is \emph{$\ww$-plurality-driven} if for every message $\mu \in \mathcal{M}$ with $f(\mu) \neq \mathbb{0}$:
  $$f(\mu) \;=\; \min \left(\argmax_{c \in \mathcal{C}} W_f(c, J(\mu))\right)$$
  In words: $f$ is $\ww$-plurality-driven, if $f(\mu)$ is always the value $c \in \mathcal{C}$ which the highest total weight of non-equivocating validators voted for in their latest non-$\mathbb{0}$ messages, using the total order on $\mathcal{C}$ as a tie-breaker.
\end{definition}

In particular, if more than half of the votes (by weight) in $L_f(\mu)$ are for a value $c$, and $f(\mu) \neq \mathbb{0}$, $\mu$ must also be: $f(\mu) = c$. So if $f$ is $\ww$-plurality-driven, the property $f(\mu) = c$ is $\ww$-majority-driven relative to $f(\mu) \neq \mathbb{0}$, in the following sense:

\begin{definition}
A property of messages $p$ is \emph{$\ww$-majority-driven relative to} a property $q$ if every valid message $\mu$ with
  $$\ww (\{\lambda \in L_q(\mu) \mid p(\lambda) \}) \;>\; \frac{1}{2} \ww(L_q(\mu)) \quad \text{ and } \quad q(\mu)$$
has property $p$. I.e. whenever $\mu$ has property $q$ and a strict majority of latest honest messages in $J(\mu)$ with property $q$ has property $p$, then $\mu$ also does.

$p$ is \emph{$\ww$-majority-driven} if it is $\ww$-majority-driven relative to the always-true property, i.e. if every valid message $\mu$ with
$$\ww (\{\lambda \in L(\mu) \mid p(\lambda) \}) \;>\; \frac{1}{2} \ww(L(\mu))$$
has property $p$. I.e. whenever a strict majority of latest honest messages in $J(\mu)$ has property $p$, then $\mu$ also does.
\end{definition}

These definitions are motivated by the blockchain case where we will prove that for every block $b$, ``the child $c$ of $b$ such that the message votes for (a descendant of) $c$'' is plurality-driven. This will imply that if $c$ is a child of $b$, the property ``the message votes for $c$'' is majority-driven relative to ``the message votes for $b$''. See section \ref{sectionBlockchain} for details. We will examine conditions under which majority-driven properties become finalized such that all future messages have them.

\begin{lemma}\label{leaderLemma}
If $f: \mathcal{M} \rightarrow \mathcal{C} \cup \{ \mathbb{0} \}$ is $\ww$-plurality-driven, $f(\lambda) \neq \mathbb{0}$, $f(\mu) \neq \mathbb{0}$, and $J(\mu) = \bar{J}(\lambda)$, then $f(\mu) = f(\lambda)$.
\end{lemma}

We will later try to construct situations like this, where all messages directly follow some ``leader'' message $\lambda$. The lemma shows that in such situations, agreeing votes are enforced.

\begin{proof}
  The only added message in $J(\mu)$ compared to $J(\lambda)$ is $\lambda$ itself, which is comparable to all other elements of $J(\mu)$, and hence cannot be part of an equivocation. So, $E(J(\mu)) = E(J(\lambda))$.

  If $S(\lambda) \in E(J(\lambda))$, then $L_f (J(\mu)) = L_f (J(\lambda))$, otherwise $L_f(J(\mu)) = L_f(\bar{J}(\lambda)) \setminus X$, where $X$ is the set containing $S(\lambda)$'s most recent non-$\mathbb{0}$ message in $J(\lambda)$, or $\emptyset$ if there is none.

  So all that changed in $L_f(J(\mu))$ compared to $L_f(J(\lambda))$ is that possibly a vote for $f(\lambda)$ was added, and if so, possibly a vote with the same weight for some $c\in \mathcal{C}$ was removed. Thus $W_f(f(\lambda), J(\mu)) \geq W_f(f(\lambda), J(\lambda))$, and $W_f(c, J(\mu)) \leq W_f(c, J(\lambda))$ for all other $c \neq f(\lambda)$.

  That means $\argmax_{c \in \mathcal{C}} W_f(c, J(\mu))$ is a subset of $\argmax_{c \in \mathcal{C}} W_f(c, J(\lambda))$ and still contains $f(\lambda)$. Since $f(\lambda)$ was the minimum of the latter, it is also the minimum of the former, hence $f(\mu) = f(\lambda)$.
\end{proof}


\subsection{Summits}

Throughout this section, $p$ and $p'$ will be fixed but arbitrary properties, and $q = \ww(\mathcal{V})/2 + t$ with $t > 0$.

\begin{definition}
A \emph{$(p, p')$-summit with quorum $q$} is a sequence of sets $C_0 \supseteq C_1 \supseteq C_2 \supseteq \ldots$ such that:
\begin{itemize}
  \item $\mu_0, \mu_2 \in C_i$ implies $\mu_1 \in C_i$, for all $p'$-messages $\mu_0 \sqsubset \mu_1 \sqsubset \mu_2$,
  \item $p(\mu)$ and $p'(\mu)$ for all $\mu \in C_0$,
  \item $E(\mu) \cap S(C_0) = \emptyset$ for all $\mu \in C_0$, and
  \item $\ww(\bar{J}(\mu) \cap C'_i ) \geq q$ for all $\mu \in C_{i + 1}$,
\end{itemize}
where $C'_i = \{\nu \in C_i \mid \nu \sqsubseteq \nu' \text{ for some } \nu' \in C_{i + 1})\}$.

The summit \emph{has level $k$} if $C_k \neq \emptyset$.
\end{definition}

So a summit represents uninterrupted streaks of $p'$-messages voting for $p$ by honest (so far) validators --- those are the first three points. Each \emph{committee} $S(C_{i+1})$ is a group of validators, each of which has confirmed that a quorum ($\geq q$) of the committee is in $S(C_i)$.

We will omit $p$, $p'$ and $q$ whenever they are clear from the context.

\begin{lemma}\label{SummitLemma}
Assume $p$ is $\ww$-majority-driven relative to $p'$. Let $(C_0, C_1, \ldots)$ be a summit, $l > 0$, $\mu$ a $<$-minimal $p'$-message with $\neg p(\mu)$ and $J(\mu) \cap C_k \neq \emptyset$. Let
$$B_k(\mu) = \{S(\zeta) \mid \text{ $\zeta < \mu$, $p'(\zeta)$, $\neg p(\zeta)$, and there exists $C'_{k-1} \ni \nu' \sqsubset \zeta$ but no $C_k \ni \nu \sqsubset \zeta$.}\}$$
be the set of validators that voted against $p$ after sending a $C'_{k-1}$ message. Then:
$$\ww(B_k(\mu) \setminus E(\mu)) + \frac{1}{2} \ww(E(\mu)) \;\geq\; t$$
\end{lemma}

The intuition behind this is that a $\lambda \in J(\mu) \cap C_k$ saw a majority of $q = \ww(\mathcal{V})/2 + t$ for $p$, but $\mu$ does not see such a majority anymore.
This can happen because $t$ of those validators (by weight) changed their vote away from $p$.
It can also happen because $s$ validators equivocate:
Those don't count as voting for $p$ anymore, but they also don't count as voters at all, so anything greater than $(\ww(\mathcal{V}) - s)/2$ would still be a majority.
That's why only $s = 2t$ equivocators could bring down the votes for $p$ below the new majority threshold $\ww(\mathcal{V}) / 2 - t$.
Of course the reason can also be a combination of equivocations and changed votes.

\begin{proof}
Since $\neg p(\mu)$, $p$ cannot have the majority in $L_{p'}(\mu)$, i.e.:
\begin{flalign*}
\ww(\{ \nu \in L_{p'} (\mu) \mid p(\nu) \}) &\leq \frac{1}{2} \ww(L_{p'}(\mu)) \leq \frac{1}{2} \ww(\mathcal{V}) - \frac{1}{2} \ww(E(\mu))
\end{flalign*}
On the other hand, if $\lambda \in J(\mu) \cap C_k$, $\ww(\bar{J}(\lambda) \cap C'_{k-1}) \geq \frac{1}{2} \ww(\mathcal{V}) + t$, and every validator in $S(\bar{J}(\lambda) \cap C'_{k-1})$ who does not count as a vote for $p$ in $\mu$ must either have equivocated or produced a vote $\zeta$ against $p$ after their message in $C'_{k-1}$. In the latter case, if $\zeta$ could see $S(\zeta)$'s $C_k$ message, $\mu$ wouldn't be minimal; thus such an $S(\zeta)$ would be in $B_k(\mu)$:
\begin{flalign*}
\ww(\{ \nu \in L_{p'} (\mu) \mid p(\nu) \}) & \geq \ww(S(\bar{J}(\lambda) \cap C'_{k-1}) \setminus E(\mu) \setminus B_k(\mu)) &\\
& \geq \ww(\bar{J}(\lambda) \cap C'_{k-1}) - \ww(E(\mu) \cup B_k(\mu)) &\\
& \geq \frac{1}{2} \ww(\mathcal{V}) + t - \ww(E(\mu)) -\ww(B_k(\mu) \setminus E(\mu)))&
\end{flalign*}
Together, these prove the inequality.
\end{proof}

\begin{theorem}
Assume $p$ is $\ww$-majority-driven relative to $p'$. Let $(C_0, C_1, \ldots)$ be a summit, $\mu$ a $p'$-message with $\neg p(\mu)$ and $J(\mu) \cap C_k \neq \emptyset$. Let
\begin{flalign*}
B_l(\mu) &= \{S(\zeta) \mid \text{ $\zeta < \mu$, $p'(\zeta)$, $\neg p(\zeta)$, and there exists $C'_{l-1} \ni \nu' \sqsubset \zeta$ but no $C_l \ni \nu \sqsubset \zeta$.}\}\\
A_l(\mu) &= B_l(\mu) \setminus E(\mu)\text{,}
\end{flalign*}
so $A_l(\mu)$ is the set of validators that $\mu$ sees voting against $p$ shortly after sending a $C'_{l-1}$ message, but not equivocating. $B_k(\mu)$ is as in Lemma \ref{SummitLemma}. Then:
$$\ww(E(\mu)) + \sum_{l=1}^k \ww(A_l(\mu)) \;\geq\; 2t(1 - 2^{-k})$$
In particular, at least $2t(1 - 2^{-k})$ validators (by weight) equivocated in any protocol state that contains both $C_0$ and $\mu$.
\end{theorem}

\begin{proof}
If $\ww(E(\mu)) \geq 2t$, the inequality holds. So assume $\ww(E(\mu)) < 2t$.
If $k = 0$, the right side is $0$, so the inequality is true.
Assume now that $k > 0$ and that the theorem holds for $k - 1$.

Note that all the $A_l(\mu)$ are disjoint:
If a validator $v$ is in both $B_l(\mu)$ and $B_m(\mu)$ with $m > l$,
then $\mu$ can see both a $\zeta$ that cannot see any $C_l$-message by $v$,
and a $\nu' \in C'_{k-1} \subseteq C_l$,
with $S(\zeta) = S(\nu') = v$.
But $\zeta \not< \nu'$, because then $\zeta$ would be between two $p$-messages in $C_0$ and thus also a $p$-message.
And $\nu' \not< \zeta$, because $\zeta$ cannot see any $C_l$-message by $v$.
Hence $v \in E(\mu)$.

For every $p'$-message $\mu' < \mu$,
$E(\mu') \subseteq E(\mu)$ and each validator in any $A_l(\mu')$ is either in $A_l(\mu)$ or in $E(\mu) \setminus E(\mu')$.
So $E(\mu') \cup \bigcup_{l=1}^k A_l(\mu') \subseteq E(\mu) \cup \bigcup_{l=1}^k A_l(\mu)$,
and hence if the theorem holds true for $\mu'$, it also holds for $\mu$.
So assume that $\mu$ is $<$-minimal with $\neg p(\mu)$ and $J(\mu) \cap C_k \neq \emptyset$.
Then Lemma \ref{SummitLemma} says:
$$\ww(A_k(\mu)) + \frac{1}{2} \ww(E(\mu)) \;\geq\; t$$
We assumed $\ww(E(\mu)) < 2t$, so $A_k(\mu) \neq \emptyset$, and there is a message $\zeta$ as in the definition of $B_k(\mu)$.
The induction hypothesis applies to $\zeta$:
$$\ww(E(\zeta)) + \sum_{l=1}^{k-1} \ww(A_l(\zeta)) \;\geq\; 2t(1 - 2^{1-k})$$
Since each element of $A_l(\zeta)$ is either in $A_l(\mu)$ or $E(\mu) \setminus E(\zeta)$:
\begin{flalign*}
\ww(E(\mu)) + \sum_{l=1}^k \ww(A_l(\mu)) &= \left(\ww(A_k(\mu)) + \frac{1}{2} \ww(E(\mu))\right) + \frac{1}{2} \ww(E(\mu)) + \sum_{l=1}^{k-1} \ww(A_l(\mu)) \\
&\geq t + \frac{1}{2}\left(\ww(E(\mu)) + \sum_{l=1}^{k-1} \ww(A_l(\mu))\right) \\
&\geq t + \frac{1}{2} \left(\ww(E(\zeta)) + \sum_{l=1}^{k-1} \ww(A_l(\zeta))\right) \\
&\geq t + t(1 - 2^{1-k}) = 2t(1 - 2^{-k})
\end{flalign*}

Finally, note that every validator in every $A_l(\mu)$ equivocated: The message $\zeta$ in the definition does not see $S(\zeta)$'s message $\zeta'$ in $C_l$, and $\zeta'$ does not see $\zeta$ because there is no vote \emph{against} $p$ between any two votes \emph{for} $p$ in $C_{l-1}$ (first point in the summit definition).
\end{proof}

Thus, once a validator has such a summit in their local protocol state $\sigma$, they know that eventually every honest validator will also see all of $\sigma$, and will only produce $p'$-messages with property $p$ from then on, unless a weight of at least $2t (1 - 2^{-k})$ validators equivocate. They can therefore consider $p$ to be \emph{finalized relative to $p'$}, with fault-tolerance threshold $2t (1 - 2^{-k})$.

Note that if we have at least a $2\ww(\mathcal{V})/3$-weight of honest validators, we could reasonably expect all of them to eventually contribute to a summit. If they persisted for enough rounds to achieve a high $k$, this would allow them to achieve a fault tolerance threshold of nearly $\ww(\mathcal{V})/3$. Thus, ignoring liveness considerations for the moment, we could hope that our system is secure against a third of the validators being faulty. In fact, in practice we might expect to be able to obtain summits with weight close to $\ww(\mathcal{V})$ if either there were few faulty validators at the time, or the faulty validators did not behave unusually. In this case, we might hope to obtain a fault-tolerance threshold of nearly everything.


\subsection{Computational Efficacy of the Finality Criterion}

\begin{theorem}
  Given a protocol state $\sigma$, properties $p$ and $p'$ and a quorum size $q > 0$, there exists a maximal valid summit $((C_i), \sigma, p, p', q)$ in the sense that for every valid summit $((\tilde{C}_i)_i, \sigma, p, p', q)$, $\tilde{C}_i \subseteq C_i$ for all $i$.

  Furthermore, there exists a polynomial time algorithm to compute $(C_i)$.
\end{theorem}

\begin{proof}
  We show that for each $k \geq 0$, there is a unique, easily computable valid summit
  $$((C_0, \ldots, C_k, \emptyset, \emptyset, \ldots), \sigma, p, p', q)$$
  so that for any valid summit $((\tilde{C}_i)_{i \in \mathbb{N}}, \sigma, p, p', q)$, we have $\tilde{C}_i \subseteq C_i$ for all $i \leq k$, and that every $p'$-message that has level at least $i$ in the latter, also has level at least $i$ in the former.

  The decreasing sequence $C_0 \supseteq C_1 \supseteq \ldots$ of finite sets must eventually be constant, and since each requirement in the definition of a valid summit affects only finitely many elements of it, this proves that $((C_i), \sigma, p, p', q)$ is also a valid summit and a maximal one.

  For $k = 0$, we let $C_0$ be the set of all validators $v \notin E(\sigma)$ whose latest $p'$-message has property $p$. Since every validator in $\tilde{C}_0$ must have at least one level $\geq 0$ message, it is clear that $\tilde{C}_0 \subseteq C_0$.

  Now assume we have constructed $C_0, \ldots, C_k$ so that they define a valid maximal summit up to $k$. Let $D_0 = C_k$, and $D_{i + 1}$ the set of all $v \in D_i$ such that there is a $p'$-message $\mu \in \sigma$ with $S(\mu) = v$ and $\ww(\{\lambda \leq \mu \mid S(\lambda) \in D_i \text{and $\lambda$ is level $k$}\}) \geq q$. Then since $D_0 \supseteq D_1 \supseteq \ldots$, there is an $i$ with $D_i = D_{i+1}$, and we set $C_{k + 1} = D_i$.

  We prove by induction on $i$ that $\tilde{C}_{k + 1} \subseteq D_i$. For $i = 0$ this is clear since we must have $\tilde{C}_{k + 1} \subseteq \tilde{C}_k \subseteq C_k = D_0$. For $i > 0$, assume $\tilde{C}_{k + 1} \subseteq D_{i - 1}$. Every $v \in \tilde{C}_{k + 1}$ has a $p'$-message of level $k + 1$, i.e. one that sees a set of level $k$ messages of weight $\geq q$ by members of $\tilde{C}_{k + 1} \subseteq D_{i - 1}$. Thus $v \in D_i$. This completes the inductive step and shows that $\tilde{C}_{k + 1} \subseteq D_i$ for all $i$, and thus $\tilde{C}_{k + 1} \subseteq C_{k + 1}$.

  It follows from the definition that every message that is level $\geq k$ in $((\tilde{C}_i)_{i \in \mathbb{N}}, \sigma, p, p', q)$ is also level $\geq k$ in $((C_i)_{i \in \mathbb{N}}, \sigma, p, p', q)$. This completes our inductive step and the proof.
\end{proof}


\section{Blockchain}
\label{sectionBlockchain}


\subsection{Blocks}

In a smart contract platform, distributed ledger, or replicated state machine, the actual value that the consensus algorithm tries to reach agreement on is an ever-growing list of transactions. In a \emph{blockchain}, that list is divided into \emph{blocks}, each of which refers to its predecessor, its \emph{parent}, by hash. The first one, the \emph{genesis block}, is part of the protocol definition, or network specification.

The set $\mathcal{B}$ of blocks is therefore a tree, with a unique finite path leading from each block to the root: the genesis block $g$. We write $\mathrm{Prev}(b)$ for $b$'s parent, so $\mathrm{Prev} : \mathcal{B} \setminus \{g\} \rightarrow \mathcal{B}$. $b_1$ is an \emph{ancestor} of $b_2$, and $b_2$ a \emph{descendant} of $b_1$, and write $b_1 \leq b_2$ if there is a sequence $b_1 = c_1, \ldots, c_n = b_2$, such that $c_k = \mathrm{Prev}(c_{k+1})$ for all $k < n$. If $n > 1$, i.e. $b_1 \neq b_2$, we say \emph{strict ancestor} resp. \emph{strict descendant}, and $b_1 < b_2$.

We recursively define the \emph{block height} as the length of the path back to genesis:
\begin{itemize}
    \item $H(g) = 0$,
    \item $H(b) = 1 + H(\mathrm{Prev}(b))$ for $b \neq g$.
\end{itemize}
Every block with height $\geq k$ has a unique ancestor with height $k$:
\begin{itemize}
    \item $A_k(b) = \mathbb{0}$ if $k > H(b)$
    \item $A_k(b) = b$ if $k = H(b)$
    \item $A_k(b) = A_k(\mathrm{Prev}(b))$ if $k < H(b)$
\end{itemize}

In Casper, every message represents a vote for a block: either one that was created earlier, or a new child of such a block. So there is a map $B : \mathcal{M} \rightarrow \mathcal{B} \setminus \{g\}$ that assigns to each message the block that it votes for such that $\mathrm{Prev}(B(\mu)) \in \{g\} \cup \{ B(\lambda) \mid \lambda < \mu\}$ for all $\mu \in \mathcal{M}$.

If a message $\mu$ votes for a pre-existing block, i.e. if there is a $\lambda < \mu$ with $B(\lambda) = B(\mu)$, then $\mu$ is called a \emph{ballot}\footnote{Since ballots ``contain'' a block that is already known from one of their justifications, the block should not be sent along again, of course. In the implementation it suffices if the ballot message contains only the hash of the block that it votes for.}. Otherwise, $\mu$ is called a \emph{block message}.

For a block $b$, let $p_b$ be the message property defined by $p_b (\mu) \Leftrightarrow b < B(\mu)$, i.e. a message has property $p_b$ if it carries a block that is a strict descendant of $b$.


\subsection{The GHOST Rule}

The set of validators and their weights can be managed ``on-chain'' by a smart contract. Every block $b$ is associated with a particular validator set that corresponds to the contract's state after $b$:

Let $\ww_b : \mathcal{V} \rightarrow \mathbb{R}_{\geq 0}$ assign to each block $b \in \mathcal{B}$ and validator $v \in \mathcal{V}$ a \emph{weight} $\ww_b(v)$. A validator $v$ is called \emph{active after} $b$, if $\ww_b(v) > 0$. We assume that after every $b$, there is at least one, but only finitely many active validators.

For theoretical purposes, assume that the $\mathrm{Hash} : \mathcal{B} \rightarrow \mathbb{N}$ function is injective, i.e. there are no hash collisions.

The GHOST rule is the requirement that for every message $\mu \in \mathcal{M}$, either $B(\mu)$ or $\mathrm{Prev}(B(\mu))$ is the GHOST choice of $J(\mu)$, where the \emph{GHOST choice} of a protocol state $\sigma$ is the unique block $a$, such that:
\begin{itemize}
    \item $a$ is a maximal element of $\{g\} \cup \{ B(\mu) \mid \mu \in \sigma\}$.
    \item For every $k < H(a)$,
      $$A_{k + 1}(a) = \min \left(\argmax_{c\in\mathcal{B}} \ww_b(\{\mu \in L_{p_b} (\sigma) \mid c = A_{k + 1}(B(\mu))\})\right)\textrm{,}$$
      where $b = A_k(a)$ and the minimum is taken with respect to the block hash.
\end{itemize}
In other words, the GHOST choice of $\sigma$ is the block that is reached by starting from $b = g$, and recursively replacing $b$ with the child $c$ of $b$ with maximal $\ww_{b} (\{\mu \in L_{p_b}(\sigma) \mid c \leq B(\mu) \})$, using the block hash as a tie-breaker. The rule says that your message must vote for a block $b$ that is either the GHOST choice itself, or its child.

Note that the GHOST rule can be verified using only the message $\mu$ itself and its justifications, but no other messages (which although they may exist, we don't need to use). Messages violating the rule can be detected before a node would insert them into its protocol state; they can simply be treated as invalid and discarded.

\begin{proposition}\label{ghostPlurality}
  Assume that the GHOST rule holds for all messages in $\mathcal{M}$. Given a block $c$ with height $k + 1$ and parent $b = \mathrm{Prev}(c)$:
  \begin{enumerate}
    \item The property $q(\mu) \Leftrightarrow c \leq B(\mu)$ is $\ww_b$-majority-driven relative to $p_b$.
    \item The function $f : \mathcal{M} \rightarrow \mathcal{B}$, defined as follows, is $\ww_b$-plurality-driven (where as a tie-breaker, blocks are ordered by hash):
      $f(\mu) = A_{k+1}(B(\mu))$ if $b < B(\mu)$, otherwise $f(\mu) = \mathbb{0}$.
  \end{enumerate}
\end{proposition}

\begin{proof}
  For point 1, assume that $\ww (\{\lambda \in L_{p_b}(\mu) \mid q(\lambda) \}) \;>\; \frac{1}{2} \ww(L_{p_b}(\mu))$ and $p_b(\mu)$. In particular $b < B(\mu)$, so $A_{k + 1} (B(\mu)) \neq \mathbb{0}$. The GHOST rule implies that
  $$A_{k + 1}(B(\mu)) = \min \left(\argmax_{c'\in\mathcal{B}} \ww_b(\{\mu \in L_{p_b} (\sigma) \mid c' = A_{k + 1}(B(\mu))\})\right)\textrm{,}$$
  and the only element in that $\argmax$ can be $c$. So $A_{k + 1}(B(\mu)) = c$ and thus $q(\mu)$.

  For point 2, assume $f(\mu) \neq \mathbb{0}$, i.e. $b < B(\mu)$. Since $f(\mu) = A_{k+1}(B(\mu))$, the second GHOST rule point implies that $f(\mu)$ is the block $c$ with the minimal hash which maximizes $\ww_b(\{\lambda \in L_{p_b} (J(\mu)) \mid c = A_{k + 1}(B(\lambda))\})$. But $L_{p_b} = L_f$ and for $\lambda \in L_f (J(\mu))$, $c = A_{k+1}(B(\lambda))$ is equivalent to $f(\lambda) = c$. So $f(\mu)$ actually maximizes $W_f(c, J(\mu))$. Hence $f$ is $\ww_b$-plurality-driven.
\end{proof}


\section{Liveness}
\label{sectionLiveness}

We analyzed how to detect finality, and how to apply that to the case of a blockchain. However, we have not specified any rules yet for when to produce messages at all, so it is not clear that new blocks get finalized, or even produced.

In this section, we present such a set of rules, and the conditions under which they guarantee liveness.


\subsection{Leaders and Ticks}

We measure time in milliseconds since the epoch\footnote{Unix time: number of milliseconds since the beginning of 1970, UTC}. An integer timestamp is called a \emph{tick}. We pseudorandomly assign one of the active validators $\mathcal{L}(i) \in \mathcal{V}$ to each tick\footnote{This needs to be done in a secure way; more on this and validator set changes later.} $i$ as the tick's \emph{leader}. To keep the proofs simple, we assume that the participants have synchronized clocks. However, a difference of $\delta$ between their clocks is equivalent to adding up to $\delta$ to resp. subtracting up to $\delta$ from message delays. If a node receives a message ``from the future'', with a timestamp greater than the current local time, it keeps the message in the buffer, and only includes it into its local state once that time has arrived.

We assume that messages have timestamps: $T: \mathcal{M} \rightarrow \mathbb{R}$, i.e. $T(\mu)$ is the time when message $\mu$ was sent\footnote{Millisecond resolution is sufficient here, but to keep notation simple, we assume exact timestamps.}.

Each validator $v$ maintains a private parameter $n_v(i) \in \mathbb{N}$ that is updated periodically. $n_v: \mathbb{N} \rightarrow \mathbb{N}$ maps each tick number to the parameter value. We will give strategies to select $n_v$ below, but we always assume that $n_v (i) = n_v (i - 1)$ unless $i$ is a multiple of both $2^{n_v(i)}$ and $2^{n_v(i-1)}$. In other words, we assume that the parameters are kept constant for time windows of $2^{n_v(i)}$ ticks, from $j$ to $j + 2^{n_v(i)} - 1$, where $j \leq i$ is maximal such that it divides $2^{n_v(i)}$. Furthermore, there are parameters $0 < R_0 \leq R_1 < 1$.

A validator $v$ will create and gossip new messages as follows. Let $i$ be the most recent tick, and $j$ the most recent tick divisible by $2^{n_v(i)}$. $v$ creates a message:
\begin{itemize}
    \item at time $j$ if $v$ is the leader of $j$,
    \item if $v$ is not $j$'s leader, as soon as it receives the tick-$j$-message $\lambda$ from $j$'s leader, and
    \item unconditionally at some point in time between $j + R_0 \cdot 2^{n_v(i)}$ and $j + R_1 \cdot 2^{n_v(i)}$.
\end{itemize}
In the first and third case, $v$ includes as justifications its full protocol state $\sigma$. In the second case, $v$ includes as justifications only $\bar{J}(\lambda)$ and $v$'s own previous message and its justifications.

So in each round lasting $2^n$ ticks, first the ``round leader'' $l$, i.e. the leader of the round's first tick, sends a message $\lambda$ to everyone. Other validators $v$ send a message $\mu^0_v$ to everyone as soon as they received $\lambda$. After a fraction $R_0$ and before $R_1$ of the round has passed, every validator $v$ sends a message $\mu^1_v$ to everyone again. The intention is for the first message $\mu^0_v$ to confirm $\lambda$'s vote and become a level-0 message, and the second one $\mu^1_v$ to confirm a sufficient number of level-0 messages, and become a level-1 message, thus forming a summit. Validators whose $n_v(i)$ is not equal to $n$ do not participate in the round in this exact way; however, as we will demonstrate below, they can still contribute to the summit.

A reasonable choice for $R_0$ and $R_1$ is $2/3$, but this should be determined experimentally and optimized for performance, together with a concrete sending time $R_0 \leq t \leq R_1$ for each validator. The liveness proof below works with any choice, and for small numbers $N$ of validators $R_0 = R_1$ is fine. The problem is that for large $N$, each level-1 message will explicitly cite every level-0 message, so the total size of those messages will be proportional to $N^2$. If instead the validators send their messages sequentially between $R_0$ and $R_1$, then only the first one has to cite the level-0 messages, and the other ones can cite a single level-1 message, reducing the overhead to a linear one, proportional to $N$.

\begin{lemma}\label{basicLiveness}
  Let $f : \mathcal{M} \rightarrow \mathcal{C} \cup \{ \mathbb{0} \}$ be plurality-driven, and $c \in \mathcal{C}$.

  Let $H \subseteq \mathcal{V}$ be a set of honest validators with $\ww(H) > \ww(\mathcal{V}) / 2$, and let $j$ be a tick that is divisible by $2^{n_{\max}}$, where $n_{\max} = \max_{v \in H} n_v(j)$. Let $n_{\min} = \min_{v \in H} n_v(j)$. Assume that $j$'s leader $l$ is in $H$, and that there is an $R' < R_0$ such that:
  \begin{enumerate}
    \item Every message $\mu$ with $S(\mu) \in H$ and $T(\mu) \leq j - (1 - R_1) \cdot 2^{n_{S(\mu)}(j - 1)}$ reaches $l$ by tick $j$.
    \item The unique message $\lambda$ with $S(\lambda) = l$ and $T(\lambda) = j$ reaches every $v \in H$ by time $j + R' 2^{n_{\min}}$.
    \item For every $v \in H$, every message $\mu$ with $S(\mu) \in H$ and $j \leq T(\mu) \leq j + R' \cdot 2^{n_v(j)}$ reaches $v$ by time $j + R_0 \cdot 2^{n_v(j)}$.
    \item $f(\mu) \neq \mathbb{0}$ for every $\mu \geq \lambda$ with $\mu \in H$.
  \end{enumerate}
  Then after tick $j + R_1 \cdot 2^{n_{\max}}$, there is a level-1 summit with $q = \ww(H)$ consisting of all members of $H$, finalizing the value $c = f(\lambda)$ relative to $f \neq \mathbb{0}$.
\end{lemma}

\begin{proof}
  Every $v \in H \setminus \{l\}$ receives $\lambda$ before $j + R' \cdot 2^{n_{\min}} \leq j + R' \cdot 2^{n_v(j)}$, so the first message $\mu^0_v$ that $v$ sends after tick $j$ has $J(\mu^0_v) = \bar{J}(\lambda) \cup \bar{J}(\mu')$, where $\mu'$ is $v$'s previous message.

  Since $\mu'$ was sent no later than $j - (1 - R_1) \cdot 2^{n_v(j - 1)}$, it reached $l$ by tick $j$, so $\mu' < \lambda$ and therefore $J(\mu^0_v) = \bar{J}(\lambda)$. By Lemma \ref{leaderLemma}, this implies that $f(\mu^0_v) = c$.

  For $l$ itself, we define $\mu^0_l = \lambda$, so $f(\mu^0_l) = c$ is also true.

  Since $T(\mu^0_v) \leq j + R' \cdot 2^{n_w}(j)$ for every $w \in H$, $\mu^0_v$ reaches every $w \in H$ before $w$ creates its second message $\mu^1_w$ after tick $j$.

  So for all $v, w \in H$, $\mu^1_w > \mu^0_v$.

  We will prove that the $\mu^0_v$ are level-0, and the $\mu^1_v$ are level-1 messages of a summit with quorum size $\ww(H)$ for the property that $f$ has value $c$. We already know that each $\mu^1_v$ can see all messages $\mu^0_w$, which have total weight $\ww(H)$, so all we have to show is that the latter are level-0 messages, i.e. that no validator $v \in H$ changes away from the value $c$ after $\mu^0_v$.

  We prove this by induction on $<$. So let $\kappa$ be a message by $v \in H$ and $\kappa \geq \mu^0_v$:

  If $\kappa = \mu^0_v$, we already know that $f(\kappa) = c$.

  So assume now that $\kappa > \mu^0_v$. Then $\kappa \geq \mu^1_v$ and hence $\mu^0_w < \kappa$ for every $w \in H$. So for every $w \in H$, $w$'s message $\xi \in L_f(\kappa)$ is at least $\xi \geq \mu^0_w$. By the induction hypothesis, $f(\xi) = c$. Therefore, in $L_f(\kappa)$ a strict majority $\geq \ww(H)$ is voting for $c$, so since $f$ is plurality-driven, $f(\kappa) = c$.
\end{proof}

In a blockchain context, leader messages $\lambda$ (first bullet point) should always carry new blocks. To minimize time to finalization, all other messages $\mu$ should be required to be ballots, and just vote for the most recent leader block: $B(\mu) = B(\lambda)$. Thus whenever the lemma's requirements are satisfied in any round, the full blockchain up to the most recent leader's block becomes finalized, because by proposition \ref{ghostPlurality}, the function $\mu \mapsto A_{H(B(\lambda))}(B(\mu))$ is plurality-driven.

That means, to have a live blockchain we just need to make sure that eventually the honest validators' $n_v$ values become large enough for the lemma to apply. On the other hand, if the $n_v$ are very large, new blocks will only be created very infrequently and the blockchain's progress will be slow; so it's important to optimize the choice of these parameters.

This is the motivation for the protocol's name: The nodes have different speed parameters $n_v$, as if they were driving on different lanes of a highway.


\subsection{Parameter strategy: Target Threshold for Finality}

Given a family $\mathcal{F}$ of voting functions, a threshold $0\% < D < 100\%$, \emph{break parameters} $C_0 < C_1 \in \mathbb{N}$, and an \emph{acceleration parameter} $B \in \mathbb{N}$, we choose $n_v$ as follows. Given $i$, and $m = n_v(i - 1)$, whenever $2^m$ divides $i$ and $n_v$ was constant between $i - C_1 \cdot 2^m$ and $i - 1$:
\begin{itemize}
  \item If $i/2^m$ is even and fewer than $C_0$ out of all $k \in \{1, \ldots, C_1\}$ succeeded, then let $n_v(i) = m + 1$, where:\\
    A $k \in \mathbb{N}$ \emph{succeeded} if there is an $f \in \mathcal{F}$ and a leader message $\lambda$ in tick $i - k \cdot 2^m$, such that $f$ was $\mathbb{0}$ for all messages in $J(\lambda)$, $f(\lambda) \neq \emptyset$, and $f(\lambda)$ was finalized with threshold $D$ in our local protocol state by time $i - (k - 1) 2^m$.
  \item Otherwise, if $i/2^m$ is divisible by $B$, let $n_v(i) = m - 1$.
\end{itemize}
In other words, if fewer than $C_0$ out of $C_1$ consecutive leaders have successfully finalized a new value with threshold $D$, then we increase $n_v$; but every $B$ rounds, we optimistically decrease it. If both criteria apply, increasing $n_v$, i.e. slowing down, takes precedence.

In practice $D$ should probably be about $1\%$: Even if in a single round, the leader's value gets finalized with only that threshold, if there are enough honest validators, its threshold will increase in the next round anyway. $C_0$ and $C_1$ should be chosen so that it is rare for $C_0$ faulty leaders to occur in any sequence of $C_1$ subsequent rounds (resulting in an unnecessary slowdown). In particular, during $B$ rounds, the expected number of such a faulty streak must be much less than $1$, e.g. $C_0 = 10$, $C_1 = 40$, $B = 1000$.

\begin{proposition}
  If there is a set $H$ of honest nodes of weight at least $(1/2 + D) \ww(\mathcal{V})$, all leaders in $H$ set a first non-$\mathbb{0}$ value for some $f \in \mathcal{F}$, and there is some upper bound such that all messages between members of $H$ are eventually delivered within that bound, then eventually some value gets finalized with threshold $D$.
\end{proposition}

\begin{proof}
  Every $C$ rounds where nothing gets finalized, each honest validator $v$ increments their $n_v$. Eventually, all members of $H$ will have slow enough rounds to satisfy the requirements of Lemma \ref{basicLiveness}.
\end{proof}

% \subsection{Parameter strategy: Multiple of a Percentile}
%
% Each validator $v$ measures the difference between incoming messages' timestamps and its local time: Let $d_{w, v}(i)$ be the delay of validator $w$'s latest message that reached $v$ by tick $i$, and $d_{v, v}(i) = 0$. Let $t_v(i)$ be $C$ times the $D$-th percentile of the values $d_{w, v} (i)$, weighted by $w$'s weight, where $C \geq 1$ and $50\% \leq D \leq 100\%$ are constants. The choice of parameters $C$ and $D$ influences the network's fault tolerance and synchrony requirements. $n_v(i) \in \mathbb{N}$ is minmial such that $2^{n_v(i)} \geq t_v(i)$, if possible, i.e. in each tick $i$, if $2^{n_v(i - 1)}$ divides $i$, then $n_v(i)$ is the greatest number such that:
% \begin{itemize}
%   \item $2^{n_v(i)}$ divides $i$, and
%   \item no integer $m < n_v(i)$ satisfies $2^m \geq t_v(i)$.
% \end{itemize}
% In other words, we eventually adjust our $n_v$ such that $2^{n_v(i)}$ is at least $C$ times the estimated time it takes for a fraction $D$ of the validators to send a message to us.
%
% \begin{proposition}
% Let $D = 50\%$. Assume that there is a set $H \subseteq \mathcal{V}$ of honest validators whose total weight is more than half of $\mathcal{V}$'s total weight, and such that the fastest message ever delivered between two members of $H$ is at most $C$ times as fast as the slowest one.
%
% Then for every $\ww$-plurality-driven $f$ that is eventually not $\mathbb{0}$, there will eventually be a summit finalizing the property $f(\mu) = c$ for some value $c$.
% \end{proposition}
%
% \begin{proof}
%   As soon as every member of $H$ has received a message from every other member of $H$, they all compute their median delays as something that is at least the fastest message's delay. So $C$ times that delay is at least the slowest message's delay. In particular, all $t_v(i)$ for large enough $i$ are longer than the delivery time from messages between members of $H$, so eventually all $2^{n_v(i)}$ are. Thus the requirements of Lemma \ref{basicLiveness} are satisfied: Eventually there will be a tick $j$ with a leader $l \in H$ such that $j$ is divisible by $2^{n_{\max}}$, finalizing a value for $f$.
% \end{proof}
%
% \begin{proposition}
%   Let $C \geq 2$, $D = 75\%$, and assume that the set $H$ of honest validators' weight is at least $75\% \cdot \ww(\mathcal{V})$. Also, assume that delivery times between each pair of honest validators don't change by a factor of more than $C / 2$ between different rounds and when reversing direction, and such that the triangle inequality is satisfied up to that factor. I.e. assume that there is a metric\footnote{I.e. $c_{v, w} = c_{w, v}$ and $c_{u, w} \leq c_{u, v} + c_{v, w}$ for all $u, v, w \in H$} $(v, w) \mapsto c_{v, w}$ on $H$ such that every message from $v \in H$ to $w \in H$ arrives with a delay of at least $c_{v, w}$ and at most $C/2 \cdot c_{v, w}$.
%
% Then for every $\ww$-plurality-driven $f$ that is eventually not $\mathbb{0}$, there will eventually be a summit finalizing the property $f(\mu) = c$ for some value $c$.
% \end{proposition}
%
% \begin{proof}
%   TBD
% \end{proof}


\subsection{Leader Selection and Weight Readjustment}

The leader-based algorithm requires all nodes to agree on the same leader schedule, which creates some difficulties. To begin with, it makes sense to assign leaders pseudorandomly, with probability proportional to weights in order to guarantee that honest leaders are selected a reasonable fraction of the time (as dishonest leaders can disrupt liveness or leave out transactions). Unfortunately, this creates a chicken-and-egg problem since the leader schedule in a network with dynamic weights will depend on the contents of some earlier block. Another issue is that an attacker might try to modify their stakes in a way that causes the known pseudorandom number generator to assign leadership to the attacker's nodes many times in a row.

To help resolve this, we will keep the validator set and their weights constant for long periods of times, e.g. for one week. We call such a period an \emph{era}. To address the above issues, we need to ensure that:
\begin{itemize}
  \item The block deciding the next era's sequence of leaders, i.e. the seed for the PRNG and the set and weights of the validators, is finalized with a very high fault tolerance by the end of an era.
  \item The validator creating that block does not have an opportunity to choose among a large number of leader sequences.
\end{itemize}

The first point means that for every era there will be a \emph{key block} that gets created and finalized a long time (e.g. five days) before the beginning of the era, which fully determines the sequence of leaders. Later blocks cannot affect that sequence anymore.

The second point means that the creator of the key block must already be limited in their choice. We will make it so that they can only choose one out of two possible leader sequences.

So the full protocol requires several additional parameters:
\begin{itemize}
  \item The length $t_e$ of an era.
  \item The delay $t_k$ between a key block and its era.
  \item The delay $t_{\mathcal{V}} > t_k$ between the first block that contributes to the determination of the new leader sequence, and its era.
  \item The time $t_f$ after the end of an era that the previous validators continue voting.
\end{itemize}

Each block whose parent has a timestamp between $k t_e$ and $(k + 1) t_e$ belongs to era number $k$.
Every block with a timestamp between $k t_e - t_{\mathcal{V}}$ and $k t_e - t_k$ contains a random bit chosen by the block's sender.

The first block with a timestamp $\geq k t_e - t_k$ is the \emph{key block} for era $k$, and the first block with a timestamp $\geq k t_e - t_{\mathcal{V}}$ is the \emph{booking block}.

The validator weights $\ww_b$ for a block in era $k$ are the ones implied by the on-chain governance smart contract state in the unique booking block that is an ancestor of $b$. (See section \ref{sectionPos} for details.)

And the leadership schedule a validator with state $\sigma$ uses during era $k$, i.e. between time $k t_e$ and $(k + 1) t_e$, assigns to each tick $i$ the outcome of a PRNG selecting a validator with weighted probabilities $\ww_b$ seeded with:
\begin{itemize}
  \item the hash of $b$ and
  \item the sequence of random bits of the blocks between $b$ and the unique key block that is an ancestor of $c$,
\end{itemize}
where $c$ is the GHOST choice of $\sigma$ and $b$ is the unique booking block that is an ancestor $c$.

Between $(k + 1) t_e$ and $(k + 1) t_e + t_f$, validators from era $k$ still participate in the protocol, even though they can't be leaders again and are only allowed to send ballots (except if they remain active in era $k + 1$ as well). This is to ensure that the last blocks of era $k$ still get finalized, even if many of the era's validators leave in era $k + 1$.

This all works well if the key block is finalized with a sufficient threshold by the time a new era begins. If that is not the case, however, the network's liveness is in danger. Therefore $t_k$ should be large enough to not only finalize the key block, but also for checkpoints to be exchanged and manually verified between the key block and the start of the era. (See section \ref{sectionCheckpoints}.)

\subsection{Independent Eras}

Several aspects of the protocol are simplified by making different eras of the protocol essentially independent of each other. What this means in practice is that each era will be essentially an independent instantiation of the Highway protocol with a fixed set of validators and weights and leader schedule (which are determined by the key block and booking block corresponding to that era). The genesis block of this era will have a state inherited from the end block of the previous era (the first block published in that era after that era's end time that is finalized to a sufficient degree of certainty).

Although this view of the era structure simplifies several things, there are some disadvantages. Firstly, it is not clear how to reward or penalize behavior from the validators of the previous era for behavior that takes place after the creation of the final block. In particular, such rewards or penalties cannot be implemented in the previous era as the final block is the last block of that era whose values affect the next era, and therefore we will need some other mechanism. Another potential disadvantage is the time lost during the transition between eras. If the validators of the new era need to wait for the previous era's end block to be finalized, they may lose time before they are able to start.

\subsubsection{Rewards and Penalties}

In order to reward or penalize validators from the previous era for their tail behavior, we will need to implement new transactions that can be called in the new era that make reference to behavior in the previous era. For example, an accusation transaction provides evidence of an equivocation of a validator in the previous era and when executed has that validator's stake slashed accordingly.

We will also need to have an initialization transaction, which includes evidence of a summit finalizing the end block of the previous era to a sufficient degree of certainty. When then transaction is processed, it causes the state of that end block to be adopted along with potentially rewards for the previous era's validators depending on their contributions to the finalization of the end block.

\subsubsection{Parallel Eras}

In order to avoid losing time between eras, we can allow a new era's validators to start creating blocks that order new transactions even before the last era's end block has been finalized. They will not be able to execute these transactions as the state of the system at the start of the era has not yet been determined. This means for example that a transaction to transfer money from $A$ to $B$ cannot be processed because it is not clear whether or not $A$ has sufficient money in their account. However, these transactions can be ordered so that once an initialization transaction is performed, the finalized transactions can be processed in order on the newly adopted state.

\section{A Permissionless Network}
\label{sectionPermissionless}


\subsection{Proof of Stake}
\label{sectionPos}

So far we proved the desirable properties of the protocol under the assumption that a certain fraction of validators (by weight) is well-behaved. A real-world network's security can approximately be measured in the cost of an attack: If I wanted to bribe the validators into reverting a finalized transaction or stalling the network, how much would I have to pay?

In a proof of stake network, these aspects are connected by making the weight proportional to a deposit --- the validator's \emph{stake} --- that is locked for a period beginning sufficiently long before the weight was assigned, and ending sufficiently long after it was unassigned. Incorrect behavior, as far as it can be determined objectively, is punished automatically by removing some amount from the deposit. Making stakes is incentivized by rewarding correct behavior.

All of this can be implemented as a smart contract running inside the system itself, with two special hooks:
\begin{itemize}
  \item The state of the smart contract determined by each block $b$ (i.e. the result of executing the transactions in all of $b$'s ancestors including $b$ itself) defines the weight function $\ww_b$ after that block.
  \item The system calls the smart contract in each block even if no user made a transaction interacting with it. In that call, the system provides the contract with information about validators' behavior.
\end{itemize}
In addition, users can make transactions that call the contract to deposit or withdraw stakes.

The contract determines the weights for era $k$ using only the deposits that have been made before $k t_e - t_{\mathcal{V}}$. Withdrawal requests must only be executed with a much longer delay: Between the era's end and the payout, checkpoints must have been finalized and exchanged so that the validator has no power anymore to revert any transactions. (See section \ref{sectionCheckpoints}.)

The exact conditions and percentages for penalties are beyond the scope of this document; they depend on external economic factors and the exact use case. In general, though:
\begin{itemize}
  \item Creating an equivocation should cause a large percentage of the stake to be confiscated, possibly 100\%: Equivocations can cause finalized transactions to be reverted, and they can only be caused by malice or severe software bugs, so they need to be extremely expensive.
  \item Liveness faults are less clear: Nodes can be offline because of network or power outages; which is indistinguishable from an attack on the network's liveness. In general, guaranteeing a certain level of reliability must be part of the validator's task and what they vouch for with their stakes. But, unrealistic requirements discourage participation in the network. One compromise is to measure the time in which a validator has produced no messages, and superlinearly, increase the percentage that is confiscated.
  \item In addition, nodes should include their values $n_v$ in their messages, as a commitment to follow the schedule with the corresponding rhythm. From that it can be computed in which slots they are expected to produce blocks, and at what points in time they must unconditionally create ballots. (See section \ref{sectionLiveness}.)
\end{itemize}

Liveness faults are indistinguishable from censorship: If a majority of validators refuses to include a minority's messages, from the protocol's perspective this looks exactly like that minority failing to send messages. To discourage censorship, it might be necessary to incur a penalty not only on the validators seemingly exhibiting the liveness fault, but also on all the other validators. The precise design of such an incentive mechanism is beyond the scope of this paper.


\subsection{Checkpoints}
\label{sectionCheckpoints}

Like all proof of stake protocols, ours is vulnerable to long range attacks: For every past era $k$ whose key block was finalized with a threshold of $x$, it is still possible for more than $x$ of its validators to collude and to rewrite history starting from that point. That means there is a growing list of past sets of validators that the network's security relies on, each of which have the opportunity to revert thousands of transactions. If the network has grown in value since then, their stakes might have been much lower than the network's current total stakes; and if they have already ceased to be validators and withdrawn and sold their stakes, they have nothing to lose anymore.

This can be addressed with out-of-protocol checkpoints, similar to \cite{buterin2014subjectivity}:

A checkpoint is a block that the user can configure to be irreversibly finalized: The node will refuse to accept any blocks that are not descendants or ancestors of every checkpoint. It will act as if a validator with infinite weight had voted for all checkpoints.

Automatically turning every sufficiently finalized block into a checkpoint is an effective defense against long range attacks. However, that comes at a cost: What happens when the block in question \emph{does} get orphaned at the precise point in time where we have turned it into a checkpoint, but most other nodes have \emph{not} yet done so?

In truth, the checkpoint functionality just shifts the consensus problem to another timescale: one of weeks or months instead of seconds or minutes. On that timescale, consensus can happen semi-automatically instead of fully automatically:

Nodes should indeed automatically turn blocks into checkpoints, but in addition, they will implement functionality to publish those checkpoints and to receive them from different sources e.g.:
\begin{itemize}
  \item One day after the end of an era, automatically make the last block of that era (according to the local GHOST rule) a checkpoint.
  \item Automatically publish your checkpoint via different channels: websites, blogs, social media.
  \item Subscribe to your friends checkpoint channels, and to those of some media you trust.
  \item Whenever there are any two checkpoints that are not ancestors of each other, alert the user: An exceptional fork has happened, or one of our trusted checkpoint sources has gone bad. At this point, the user needs to remove the untrusted checkpoints and subscriptions, and manually choose a fork.
\end{itemize}

\section{Spam}

In a proof of work system, it is usually easy to control the amount of work that needs to go into processing and transmitting new messages. This is because a new block can be produced only at great computational cost, usually enough that it will take substantially fewer resources to process and propagate this block than were needed to produce it in the first place. However, in a proof-of-stake system, this no longer needs to be true. In fact, one could imagine an adversary trying to flood the system with new blocks (which are now computationally easy to manufacture), hoping that some or all of the other validators will not be able to cope with the stream. This is a possible form of denial of service attack against the system.

Unfortunately, it is non-trivial to deal with such attacks. One cannot simply ignore votes from a validator who is sending too many of them. For one, the validator who is originating the votes may not be the one who you are receiving them from. If these votes are contained in the justification of a block sent by a confederate, it may not be clear whether the confederate is actively assisting the denial of service attack or if they simply had a greater download capacity than you do.

This problem is made worse by noting that the honest validators must essentially agree on which votes they are going to process. If validator $A$ downloads one of these spammy votes and incorporates it into one of their own justifications, other validators will not be able to process $A$'s vote until they have also processed these other messages. This is perhaps even worse if an attacker sends different sets of spam votes to each of the honest validators. If each honest validator includes a reasonable subset of these messages in their next block, it may not be apparent that the system is being overtaxed until they have a chance to look at each others' blocks and realize that they will have a huge number of new votes that need to be downloaded and processed before they can continue.

In this section, we first outline two of possible spam patterns that creates very high amounts of messages, in order to show that without taking any preventive measures, the spam attacks can cause a resource depletion in validator nodes and hence possibly crash the whole protocol. Then, we show how using relatively simple techniques, it is possible to bound the amount of possible spam to the easily managable size. 

\subsection{Classification of Basic Spam Patterns}

In this subsection, we lay out two basic categories of spam attacks:

In a vertical spam, the attacker does not produce any equivocations and merely produces a single long string of sequential messages. This could be done by simply setting their round exponent to an unrealistically small number. This is substantially the easier type of spam to deal with, but the only type that can be produced without the attacker equivocating, and thus allowing them to be readily punished for it.

In an equivocation bomb, or a fork bomb\cite{gkagol2019aleph}, instead of making many sequential messages, the attacker makes a large number of non-sequential messages. This is a somewhat trickier situation to deal with especially because as discussed above, the attacker can show different messages to different validators, and it will take some coordination between the other validators in order to determine which messages should and shouldn't be ignored.


\subsubsection{Vertical Spam}

In vertical spam, an attacker simply creates a long sequence of valid votes, faster than the other validators can readily handle. Perhaps the easiest way to deal with this issue is to set a minimum round exponent that is not too much smaller than what one would expect the fastest rate that the system can process messages. If additionally it is required that messages be timestamped and that no validator sends more than two messages in a round (measured by comparing the timestamp of the message to the timestamp of the third-most-recent message by the same validator in its history), this ensures that no attacker can create valid, non-equivocating messages at a rate much faster than that of any of the honest validators.

This has one potential issue, however. Although an attacker many not be able to produce messages at a faster rate than honest validators, by delaying sending these messages, they may be able to manufacture a situation where many saved up messages are released at once. If they spent enough time saving up this sequence of messages, this may well be enough to at least temporarily shut down the system.

\subsubsection{Equivocation Spam}

Perhaps the more challenging mode of spam involves equivocation. Instead of sending a large number of sequential votes, an attacker could send a large number of non-sequential votes. These would by definition be equivocations for which the attacker might be punished by the protocol, but they could potentially be disruptive nonetheless.

In the most basic form of this attack, a single malicious validator sends a very large number of equivocating votes to the other validators. If the other validators start to include all of these votes in their own blocks, the attack could occupy as much bandwidth as vertical spam, but without being rate limited by the round exponent. We note that this kind of direct attack is fairly easy to avoid simply by having validators refuse to directly cite equivocating votes in their justifications (we will discuss a slightly more sophisticated version of this idea shortly). This ensures that each honest validator only directly cites a single chain of messages by each other validator. However, this still means that an equivocating attacker can send a different chain of messages to each honest validator.

The problem potentially much worse than this if several malicious validators are willing to work together. Assume that you have validators $A_1,A_2,B_1,B_2,C_1,C_2,\ldots.$ These validators can work together in order to create the following collection of messages. $A_1$ will create a vote $m_1$ and $A_2$ creates a vote $m_2$. $m_1$ will directly cite votes $m_{11}$ and $m_{12}$ by $B_1$ and $B_2$, respectively, while $m_2$ will cite equivocating votes $m_{21}$ and $m_{22}$. In turn message $m_{ij}$ will cite votes $m_{ij1}$ and $m_{ij2}$ by $C_1$ and $C_2$. Continuing this pattern, a conspiracy of $2k$ validators can create a pattern of $2^k$ votes over the course of $k$ rounds so that no vote directly cites an equivocation. An honest validator trying to validate the top messages in this pattern will need to download exponentially many equivocating messages at the bottom. What is perhaps worse here is that although it will be easy for the honest validators receiving these messages to tell that something is going wrong (given the extraordinary number of messages and equivocations), it is non-trivial to determine exactly who is responsible and where to start cutting off the bad votes. For example, in the pattern described, $A_1$ and $A_2$ actually have not equivocated and so cannot be distinguished from honest validators. This means that a validator $V$ receiving this pattern will be left with the choice of either ignoring these messages (which means that if $A_1$ and $A_2$ were honest that these validators are being permanently cut off from each other), or forwarding them (which can be a problem if a version of this with different equivocating messages was sent to the other honest validators).

To prevent these issues, we will need to be extra suspicious of votes that cite other votes that equivocate with ones that we already know of. In particular, we will want to know that these votes are not themselves equivocations. We resolve this by introducing a system of endorsements.


\subsection{Spam Prevention Techniques}

We begin by introducing some new definitions.

%\subsubsection{Chains and Endorsements}

\begin{definition}
A \emph{chain} is a totally ordered sequence of messages by the same validator. Equivalently, it is a set of messages by a single validator that contains no pair of equivocating messages.
\end{definition}

Notice that if we limit the round exponents that can be used by any validators and require that they send no more than three sequential votes per round, this guarantees that no chain will contain too many messages. Our goal will thus be to ensure that we never need to include too many chains of messages in our justifications. We note that one equivocating validator might send equivocating votes to each of the other $n$ validators and if they do not coordinate about which votes to include in their next justification, they may be forced to include $n$ chains from this validator in order to incorporate each others' votes. However, we will want to ensure that the situation can not be made to be much worse than that by some kind of equivocation bomb, and this will require that the honest validators coordinate on which messages to include. We do this using a system of endorsements described below.

\begin{definition}
An \emph{endorsement} is a non-vote message between validators that consists of a validator $V$ and the hash of a vote $\lambda$, and is digitally signed by $V$. This endorsement asserts that $V$ did not know of equivocations by the sender of $\lambda$ at the time.

A vote $\lambda$ is said to be \emph{endorsed} if it has endorsements by a collection of validators of total weight more than $2/3$ of the total validator weight.
\end{definition}

The protocol will specify that validators create endorsements only of votes sent by other validators that they do not know to be equivocators. We will discuss later the circumstances under which we will actually need them to create endorsements, but for now, we will assume that they create endorsements for every message they receive by a sender that they do not know to be an equivocator. Once an endorsement is created, it can then be shared among the other validators.

We note that this means that an honest validator will never endorse a pair of equivocating votes as by the time they receive the second of these votes, they will know the sender to be an equivocator and thus will not endorse. In fact, exhibiting endorsements of equivocating messages by a single validator is verifiable proof of bad behavior by that validator for which they can be slashed. Furthermore, assuming that the honest validators make up at least $2/3$rds of the total validator weight, this means that no two endorsed votes can be equivocations. This is because for any two endorsed votes $\lambda_1$ and $\lambda_2$, there must be some honest validator that endorsed both of them.

We will often think of endorsed votes as being certified by the validators as a whole. While the justifications of other messages might be considered suspect (in that they might contain equivocation bombs), endorsed votes are considered to be more solid.
\begin{definition}
A vote $\mu$ is said to cite another vote $\lambda$ \emph{naively}, denoted $\mu \geq_n \lambda$ if $\mu \geq \lambda$ and there is no endorsed vote $\nu$ so that $\mu > \nu \geq \lambda$.
\end{definition}

We now introduce a new validity criterion:

\paragraph{Limited Naivety Criterion:} A vote $\mu$ sent by validator $V$ is considered incorrect if there is an equivocation $(\lambda_1, \lambda_2)$ and two messages $\mu_1, \mu_2$ (possibly equal, possibly coinciding with $\mu$) by $V$ such that $\mu \geq \mu_1 \geq_n \lambda_1 and \mu \geq \mu_2 \geq_n \lambda_2$.

Perhaps the most important property here is that this is sufficient to show that not too many equivocating votes are contained in any message justification.

More specifically, the following theorem states that if we additionally bound the round exponent, we can ensure that no message chain is too long, and thus this will limit the number of messages that ever need to be processed during one era.

\begin{theorem}\label{thm:limit_spam}
Let $k_{min}$ be the minimum round exponent, and $2^K$ be the length of an era. Additionally, let $n$ be the total number of validators among which there are $d$ dishonest ones with cumulative weight strictly less than $2\ww(\mathcal{V})/3$. Then, any given message $\mu$ created by an honest validator cites at most $2^{K-k_{min}}\cdot (d\cdot n + n-d)$ messages.
\end{theorem}
\noindent 
The following subsection is devoted to proving Theorem~\ref{thm:limit_spam}.

\subsubsection{Bounding the Amount of Spam}

\begin{lemma}\label{chainBoundLemma}
Consider a protocol state $\sigma$ in which all votes are valid according to Limited Naivety Criterion. Let $n$ be the total number of validators among which dishonest ones have cumulative weight strictly less than $2\ww(\mathcal{V})/3$. Then for any message $\mu$ and any validator $W$, the set of messages by $W$ in the justification of $\mu$ are contained in the union of at most $n$ chains of messages.
\end{lemma}

\begin{proof}
As there are less than $2\ww(\mathcal{V})/3$ dishonest validators and no honest validator will ever endorse an equivocation, no pair of equivocating votes are endorsed in $\sigma$. 

Let $V$ be the sender of $\mu$.

We divide the votes sent by $W$ in the justification of $\mu$ into groups as follows:
\begin{itemize}
\item For each validator $U \neq V$, we define $C_U$ to be the set of votes $\lambda$ by $W$ in the justification of $\mu$ so that
$\lambda$ is naively cited by an endorsed vote $\nu < \mu$ sent by $U$.
\item We define $C_V$ to be the set of messages $\lambda$ sent by $W$ that are naively cited by a message $\mu' \leq \mu$ with $\mu'$ sent by $V$.
\end{itemize}

We need to establish two claims:
\begin{enumerate}
\item Every vote by $W$ in the justification of $\mu$ is in $C_U$ for some validator $U$ (possibly $U=V$).
\item Each $C_U$ is a chain.
\end{enumerate}

For the first claim, consider a vote $\lambda < \mu$ sent by $W$. Find a minimal message $\nu$ with $\mu \geq \nu \geq \lambda$ so that $\nu$ is either endorsed or equal to $\mu$ (such a $\nu$ must exist as $\mu$ is such a message). We note that the minimality implies that $\lambda$ is naively cited by $\nu$ as if there were an endorsed $\nu'$ with $\nu > \nu' \geq \lambda$, $\nu$ would not be minimal. If $\nu=\mu$, then by definition $\lambda$ is in $C_V$. Otherwise, $\nu$ is endorsed and therefore $\lambda$ is in $C_{S(\nu)}$.

For the second claim, we note that by Limited Naivety Criterion, $C_V$ contains no equivocations (as each vote in it is naively cited by a message from $V$), and is therefore a chain. For $U \neq V$, by assumption the endorsed votes by $U$ contain no equivocations and are therefore a chain. This means that among the endorsed votes by $U$ in the justification of $\mu$, there is a maximal one, say $\mu_U$ so that every other endorsed vote by $U$ in the justification of $\mu$ is also in the justification of $\mu_U$. It then follows that every vote in $C_U$ is naively cited by a vote of $U$'s in the justification of $\mu_U$. Therefore, by Limited Naivety Criterion, there can be no equivocations among such votes, and therefore they form a chain.

This completes our proof.
\end{proof}


Intuitively, Lemma \ref{chainBoundLemma} provides a bound on the number of messages that can be created by means of equivocation spam. This is enough to prove Theorem~\ref{thm:limit_spam}.




\begin{proof}[Proof of Theorem~\ref{thm:limit_spam}]
During the timespan of a single era, each chain can grow to the height of at most $2^{K-k_{min}}$, as no validator is allowed to create messages more often than the minimum round exponent.

Due to the Lemma \ref{chainBoundLemma}, in $J(\mu)$ there can be at most $n$ chains of each of at most $d$ dishonest validators, and additionally one chain of each of the remaining validators (as all messages of an honest validator are bound to form a chain), so $(d\cdot n + n-d)$ chains in total.

In summary, at most $2^{K-k_{min}}\cdot (d\cdot n + n-d)$ messages can be in $J(\mu)$. 

\end{proof}

\subsubsection{Liveness After Adding Spam Prevention Measures}

While the application of Limited Naivety Criterion ensures that an attacker cannot force the honest nodes to process too many votes, it is no longer clear that it can be made to work with our original liveness strategy as that required validators to send votes containing all votes they know of, which may well violate Limited Naivety Criterion. We thus will need to modify our liveness strategy slightly.

The basic idea of how to do this is that when a validator $V$ receives a vote $\lambda$ that cannot immediately be included in their next votes's justification (due to a violation of Limited Naivety Criterion), they will ignore the vote, placing it in a verification buffer until some later condition is satisfied. When sending their next message, $V$ will pretend that these buffered votes were never received. In order to ensure that this method maintains liveness, we will need it to satisfy two conditions:
\begin{enumerate}
\item No honest validator ever sends a vote violating Limited Naivety Criterion.
\item Assume that the honest validators account for at least $2/3$rds of the total validator weight. Assume also that there is a period of network stability during which any communication between honest validators is received within time $\Delta$. Then during this period any vote sent by an honest validator will be received by each other honest validator and removed from their verification buffers within time $O(\Delta)$.
\end{enumerate}
Together these imply that only votes satisfying Limited Naivety Criterion are sent and that if a vote is not considered to be delivered until it is removed from the verification buffer of the receiver, under the assumptions of condition 2 above, votes are being delivered within bounded time between honest validators, which is sufficient for our liveness strategy.

The strategy that satisfies these conditions is relatively simple. An honest validator $V$ maintains a list $E_V$ of other validators for which $V$ knows of equivocating votes sent by that validator. When $V$ receives a new vote $\mu$ it is put in $V$'s verification buffer if either:
\begin{itemize}
\item The sender of $\mu$ is a known equivocator (i.e. $S(\mu) \in E_V$).
\item The justification of $\mu$ contains some other vote already in $V$'s verification buffer.
\end{itemize}
Additionally, upon receiving any vote created by an honest-so-far validator that has an equivocation in its justifications (and hence is ''problematic'', as it can end up in the verification buffer of some honest validator), $V$ will automatically broadcast endorsement for such a vote. 
When $V$ receives endorsements for any vote $\mu$ by a set of validators comprising at least $2/3$rds of the total validator weight (i.e. $V$ receives confirmation that $\mu$ is endorsed), $V$ will remove $\mu$ and all votes in its justification from their verification buffer.

We now need to show that this strategy satisfies our requirements. First, note that since $V$ will only endorse votes by senders not known to be equivocators, $V$ will never endorse a pair of equivocating votes. This is because upon initial receipt of the second vote, $V$ will know the sender to be an equivocator and thus will not endorse. On the other hand, an honest validator will never equivocate and thus will never be on any other honest validator's list of equivocators. Thus, if endorsements are solicited for a vote $\mu$ sent by an honest validator $V$, all other honest validators will eventually endorse. Assuming that these honest validators comprise at least $2/3$rds of the total validator weight, this means that $\mu$ will eventually be endorsed. Furthermore, during periods of network stability, this will happen in time at most $O(\Delta)$. Thus, if an honest validator $V$ sends a vote $\mu$ to another honest validator $W$, it will take at most a constant number of rounds of communication for $W$ to verify the validity of the vote and at most a constant number of rounds to endorse it (if necessary), by which point $W$ will be ready to include $\mu$ in their next vote.

We have left to show that an honest validator will only send messages satisfying Limited Naivety Criterion. This is because any message that clears $V$'s verification buffer either:
\begin{enumerate}
\item Was initially put into the verification buffer, but was (or was cited by) a vote that was later endorsed.
\item Was sent by a validator who was not at the time in $E_V$.
\end{enumerate}
We note that the former type of votes are never naively cited by messages from $V$ and thus do not affect Limited Naivety Criterion. We claim additionally that votes of the latter type in $V$'s justifications contain no equivocations. This is because if $V$ receives two equivocating votes, say, $\lambda_1$ and $\lambda_2$, then by the time $V$ receives the second of these votes, the sender will be put in $E_V$ and the message will end up in the verification buffer. This shows that all of $V$'s messages under this rule satisfy Limited Naivety Criterion.


%\subsection{Impatience}

%There are two substantial and related weaknesses with the above anti-spam strategy and both involve validators sending many messages without being cited by the rest of the network. The first was discussed above. That although a validator cannot create too long a chain of votes, it can withhold its votes for a long time and suddenly release a large number of them at once, perhaps momentarily flooding the network. This could be solved if we forced the validator to wait until its old messages had been processed by at least some honest validators before creating too many new ones.

%The second issue is that although Criteria 1 is sufficient to prevent the honest validators from having to keep track of more than $n$ chains from each dishonest validator, they might actually need to have to keep track of these extra chains potentially forever (or at least until the end of the era). Fixing this requires a couple of insights. Firstly, we note that our liveness strategy actually lets us impose a slightly more stringent validity criteria on messages.

%\paragraph{Criteria 2:} A vote $\mu$ satisfies Criteria 2 if and only if for every message $\lambda$ naively cited by $\mu$, $S(\lambda)\not\in E(\mu')$ for any $\mu'<\mu$ with $S(\mu')=S(\mu)$.

%In other words, you should not naively cite messages by validators that you knew to be equivocators as of your last message. It is easy to see that our liveness strategy ensures that all messages sent by honest validators satisfy this criteria, and thus we can impose it as an additional validity criteria on messages without affecting liveness. This however, means that once a validator $U$ learns that another validator $W$ is an equivocator, they will stop naively citing $W$'s messages and the chain $C_U$ from the proof of Proposition \ref{chainBoundProp} will cease to grow. If $U$ is an honest validator, we expect this to happen fairly quickly, and we will not need to worry about having to keep track of these extra chains for very long. We might have to worry though about dishonest validators.

%In particular, one could imagine dishonest validators $A,B,C,D,\ldots$ and $\alpha,\beta,\gamma,\delta,\ldots$. We could imagine that each of $A,B,C,D,\ldots$ equivocates and sends a different chain of messages to each of $\alpha,\beta,\gamma,\delta,\ldots$ who then forward them to all of the honest validators. As long as $\alpha,\beta,\gamma,\delta,\ldots$ ignore votes from each other and from the honest validators, they can continue to include these new messages from $A,B,C,D,\ldots$. Using this strategy an attacker with control of $2m$ validators can create $m^2$ chains of messages that must be processed by all of the honest validators.

%We could solve this issue by forcing the validators $\alpha,\beta,\gamma,\delta,\ldots$ to include votes from many other validators in their new votes, thus forcing them to eventually acknowledge that $A,B,C,D,\ldots$ are equivocating.

%\subsubsection{Audibility}

%To deal with the first issue, we need to make sure that a validator cannot send too many messages without these messages being received by a decent fraction of the others. For this we introduce a new validity criteria:

%\paragraph{Criteria 3:} A vote $\mu$ sent by a validator $V$ satisfies Criteria 3 if either:
%\begin{itemize}
%\item Let $\mathcal{C}$ be the collection of validators $W$ so that $\mu$ cites a message $\lambda$ by $W$, which cites a $\mu'$ by $V$ where the time between $\mu'$ and $\mu$ is at most $N$ of $\lambda$'s round lengths. Then $\mathcal{C}$ consists of validators who along with $V$ comprise at least $1/3$ of the total validator weight.
%\item $\mu$ is endorsed.
%\end{itemize}

%We note that during periods of network stability that if $N$ is sufficiently large that this should rarely be a constraint, as each validator should have seen messages from each other well within $N$ rounds. We also note that if at least $2/3$ of the validator weight is honest and if we are in a period of network stability, validators can modify their liveness strategy to accommodate this extra condition even after an extended period of network instability. Hopefully, most messages should satisfy the initial condition (as many other validators will have seen and cited their other recent messages), but if not, before sending $\mu$, $V$ can first solicit an endorsement, which ought to arrive in a timely manner.

%This also defeats the delayed message attack since the attacker will not be able to create more than about $N$ rounds of messages before they need some of these messages to either be endorsed or be cited by at least $1/3$ of the validator weight. In either case (assuming that at least $1/3$ of the validators by weight are honest), at least one honest validator will have learned of these messages and they will communicate them to the others. So assuming network stability, no validator will be able to create too many messages without them being seen by the honest validators.

%\subsubsection{Ensuring Validators are Up to Date}

%In order to ensure that none of the duplicate chains of votes produced by an equivocation remain active for very long, it is important to keep validators up to date, and in particular ensure that if any validator $W$ is known to be an equivocator by any honest validator that all other validators will be forced to acknowledge this fact (and thus by Criteria 2 stop naively citing new messages by $W$) after a small number of rounds. In order to enforce this, we introduce one final validity criteria:
%\paragraph{Criteria 4:} A vote $\mu$ sent by a validator $V$ satisfies Criteria 4 if the following holds. Let $\mu'$ be the $N^{th}$-to-last message sent by $V$. Let $\mathcal{C}$ be the set of $V$ along with other validators $W \not\in E(\mu)$ so that if $\lambda$ was the most recently sent message by $W$ in $\mu$'s justification, then the time that $\mu'$ was sent was no later than $N$ of $\lambda$'s round lengths after $\lambda$. Then $\mu$ satisfies Criteria 4 if and only if the weight of the validators in $\mathcal{C}$ is at least $1/3$ of the total validator weight.

%Assuming that the honest validators comprise at least $2/3$rds of the total validator weight, it is clear that any message satisfying Criteria 4 contains in its justification a relatively recent message sent by each validator in $\mathcal{C}$, which by the weight restriction must contain at least one honest validator. Since any honest validator learning about an equivocation will quickly (assuming network stability) be communicated to other honest validators, this means that if messages are being delivered on time, then any vote satisfying Criteria 4 must acknowledge an equivocating validator not too long after the first honest validator becomes aware of such an equivocation. This along with Criteria 2 ensures that these duplicate chains will not persist for very long.

%As is the case with Criteria 3, Criteria 4 shouldn't be much of a restriction on messages by honest validators during periods of network stability as honest validators ought to learn of each others' messages well within $N$ round lengths. However, we would also like to know that once the network recovers from an arbitrarily bad period of instability that upon return to a state where messages are reliably delivered within a period $\Delta$, which is a sufficiently small multiple of a round length, then after at worst a short startup, Criteria 4 will once again cease to be an obstacle to liveness of our network.

%As a first step in showing this, we would like to know that after network stability has been restored and all honest validators have had a chance to exchange their old messages with each other that at least some of them will be allowed to send new messages. For this we sort the honest validators by the time of their most recent message and note that any validator, $V$, not in earliest half of honest validators by validator weight, will have at least a $1/3$-validator mass of other honest validators who have more recent latest messages. Once $V$ has received these latest messages, $V$ will be able to start sending new messages without violating Criteria 4.

%Furthermore, the total weight of such validators $V$ comprise at least $1/3$ of the total validator mass. Therefore, once all of these validators have sent new messages, any other validator that receives them within $N$ round lengths will be able to start sending new messages satisfying Criteria 4 as well. Therefore, so long as messages continue to be delivered reliably within a round length and so long as the worst ratio between round lengths is no worse than $N$, adherence to Criteria 4 should not affect liveness.

\newpage

\printbibliography

\end{document}
